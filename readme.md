# NTU Class Crawler - å°å¤§ä¸Šèª²æ•™å®¤çœ‹æ¿å·è·‘èª²è¡¨æ•´ç†å°å·¥å…·

+ [NTU Class Crawler - å°å¤§ä¸Šèª²æ•™å®¤çœ‹æ¿å·è·‘èª²è¡¨æ•´ç†å°å·¥å…·](#ntu-class-crawler---å°å¤§ä¸Šèª²æ•™å®¤çœ‹æ¿å·è·‘èª²è¡¨æ•´ç†å°å·¥å…·)
  + [Introduction](#introduction)
    + [Environment](#environment)
  + [How to Use](#how-to-use)
    + [GUI (Recommended)](#gui-recommended)
      + [Demo](#demo)
      + [1. Execute crawl_tk.exe directly (For Windows only)](#1-execute-crawl_tkexe-directly-for-windows-only)
      + [2. Create virtual env in Anaconda (For Windows/Mac/Linux)](#2-create-virtual-env-in-anaconda-for-windowsmaclinux)
    + [Terminal](#terminal)
      + [Optional Arguments](#optional-arguments)
      + [Examples](#examples)
    + [Local Wep App Deployment](#local-wep-app-deployment)
      + [Demo](#demo-1)
      + [Steps](#steps)
## Introduction

é›–ç„¶[ä¸Šèª²æ•™å®¤çœ‹æ¿](http://gra206.aca.ntu.edu.tw/classrm/index.php/acarm/webcr-use1-new)æœƒå…ˆå·è·‘ä¸‹å­¸æœŸçš„ä¸€äº›èª²è¡¨ï¼Œä½†å› ç‚ºç³»çµ±è¨­è¨ˆçš„é—œä¿‚ï¼ŒæŸ¥æ‰¾å…¶å¯¦ä¸æ˜“ã€‚æ­¤ç¨‹å¼æœƒå¾[ä¸Šèª²æ•™å®¤çœ‹æ¿](http://gra206.aca.ntu.edu.tw/classrm/index.php/acarm/webcr-use1-new)ä¸Šé¢çˆ¬å–ä¸‹å­¸æœŸçš„èª²è¡¨ï¼Œä¸¦æœ‰æ¢ç†åœ°æ•´ç†ï¼Œå› æ­¤å¯ä»¥åœ¨å°å¤§èª²ç¨‹ç¶²æ›´æ–°å‰å°±å…ˆå¤§è‡´å®‰æ’ä½ çš„èª²è¡¨ã€‚é›–ç„¶ä¸Šé¢çš„è³‡è¨Šä¸å®Œæ•´ï¼Œä½†è‹¥æ˜¯æƒ³è¦äº‹å…ˆå®‰æ’èª²è¡¨çš„è©±é‚„æ˜¯ç›¸ç•¶æ–¹ä¾¿ã€‚

This is a program that can crawl data of classes in next semester from
"classroom management system[ä¸Šèª²æ•™å®¤çœ‹æ¿](http://gra206.aca.ntu.edu.tw/classrm/index.php/acarm/webcr-use1-new)". With this program, you can arrange your classes before they come out at NTU online. While the information is not quite
complete on the website, it is still a useful and helpful tool if you want to
organize the curriculum for next semester in advance. 

ä½ å¯ä»¥é¸æ“‡ä½¿ç”¨åœ–å½¢åŒ–ä»‹é¢ï¼Œä¹Ÿå¯ä»¥ç›´æ¥åœ¨ terminal ä¸­åŸ·è¡Œç¨‹å¼ã€‚æœ€å¾Œï¼Œä¹Ÿæœ‰æä¾›[ç°¡æ˜“ç¶²é ç‰ˆ](https://share.streamlit.io/icheft/ntuclasscrawler/app.py)ï¼Œå¯ä»¥è®“ä½ ä¸ç”¨ä¸‹è¼‰æ•´ä»½ç¨‹å¼ç¢¼ä¹Ÿèƒ½é †åˆ©æŸ¥æ‰¾èª²è¡¨ã€‚

You can execute this program either in GUI or terminal. In addition, the website version of this program is served [here](https://share.streamlit.io/icheft/ntuclasscrawler/app.py) so that you won't have to clone and build from scratch.



### Environment
<a target="_blank" href="https://www.python.org/downloads/" title="Python version"><img src="https://img.shields.io/badge/python-%3E=_3.7-green.svg"></a> and the following libraries are required.


> <img src="https://img.shields.io/badge/python-requests %7C beautifulSoup4 %7C tkinter %7C openpyxl %7C lxml %7C pandas-blue">


## How to Use
### GUI (Recommended)
#### Demo
<img src="./img/sample_GUI.gif">

å…±æœ‰å…©ç¨®æ–¹æ³•å¯åŸ·è¡ŒGUIç¨‹å¼ã€‚

There are two ways to use this program.

#### 1. Execute crawl_tk.exe directly (For Windows only)
> **æ­¤æ–¹æ³•ä¸ç”¨è£pythonä¹Ÿå¯ä»¥ä½¿ç”¨ï¼Œç›´æ¥åŸ·è¡Œå°±å¥½å¾ˆæ–¹ä¾¿ï¼Œæ¨è–¦ä¸€èˆ¬äººä½¿ç”¨**

ç›´æ¥ä¸‹è¼‰```dist/crawl_tk.exe```ä¸¦åŸ·è¡Œå³å¯ã€‚

Download ```dist/crawl_tk.exe``` and execute it directly.

#### 2. Create virtual env in Anaconda (For Windows/Mac/Linux)
ä¸‹è¼‰```crawl_tk.py```ä»¥åŠ```requirements.txt```å¾Œï¼Œåœ¨ Anaconda ä¸­å»ºç«‹ python 3.7 ç’°å¢ƒã€‚ä¸¦åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤

After you download the files, create a virtual env with python=3.7 in Anaconda. Then, execute

```
pip install -r requirements.txt
python crawl_tk.py
```

### Terminal 

<img src="./img/sample.gif">

```
python3 crawl.py -b [building] --semester [semester]
```

#### Optional Arguments
```
optional arguments:
  -h, --help            show this help message and exit
  --semester SEMESTER   Select the semester you want to query
  --delay-time DELAY    Set the delay time between each request
  -p PAGE, --page PAGE  Assign the maximum page of each day.
  -s [SAVE], --save [SAVE]
                        Store the result. You can specify your filename.
                        Please use .xls or .xlsx as filename extension.
  -b {1,2,3,4,5,6,7,8,9,A,B,å…±åŒ,æ™®é€š,æ–°ç”Ÿ,ç¶œåˆ,åšé›…,%}
                        Specify the building you want to query.
                        If the building belongs to a college(ex:College of Electrical Engineering and Computer Science),
                        use the code of that college(ex:9),or simply type the name of the building(ex:åšé›…).
                        The percentage sign means search for all buildings.
                        The default is set as 9.
  --search-opt SEARCHOPT

                        Comma separated values to specify search options e.g. "Title=ç©é«”é›»è·¯,Classroom=é›»äºŒ"
                        The available args include:
                            "Id": Curriculum Identity Number
                            "Class": The class number. If the course is taught by only one teacher, it is set to 00
                            "Title": Course title
                            "Instructor": Teacher name
                            "Classroom": Schedule Classroom 
                            "Time": The time of course

                        For example, if you type "--search-opt Title=ç©é«”é›»è·¯,Classroom=é›»äºŒ", you may get the following result:
                                    Id Class        Title Instructor Classroom    Time
                        0  943U0010    00       ç©é«”é›»è·¯æ¸¬è©¦        æå»ºæ¨¡     é›»äºŒ146  äºŒ2,3,4
                        1  921U9590    00  é›»åŠ›é›»å­èˆ‡ç©é«”é›»è·¯æ§åˆ¶        é™³æ™¯ç„¶     é›»äºŒ225  äºŒ7,8,9
                        2  943U0120    00     å°„é »ç©é«”é›»è·¯è¨­è¨ˆ        é™³æ€¡ç„¶     é›»äºŒ104  ä¸‰2,3,4
                        3  90140500    00       ç©é«”é›»è·¯è¨­è¨ˆ        ç›§å¥•ç’‹     é›»äºŒ229  ä¸‰7,8,9
                        4  942U0120    00     å¾®æ³¢ç©é«”é›»è·¯å°ˆé¡Œ        æ—å¤ä½‘     é›»äºŒ101  å››7,8,9


```
You can view all help message by typing
```
python3 crawl.py -h
```
#### Examples
> The default of the building is set as 9 (EECS). Remener to specify the -b option.

If you want to find the class whose name is æ©Ÿå™¨å­¸ç¿’ in EECS and store the result to excel with filename <code>ML.xls</code>, you can use
```
python3 crawl.py --search-opt Title=æ©Ÿå™¨å­¸ç¿’ --save ML.xls -b 9 --semester 1101
```
or maybe you just want to find the classes teached in Bioresources and Agriculture college, and save it locally. You can just type
```
python3 crawl.py -b 6 --save Agri.xlsx --semester 1101
```
A common way of using this program is to download all of the courses into a xlsx file, and you can search directly in the xlsx file locally.
(It may take a while to execute it, since you want to go through all of the pages in the website.)
```
python3 crawl.py -b % --save course.xlsx --semester 1101
```

### Local Wep App Deployment

å¦‚æœç§æœå™¨ï¼ˆç›®å‰è³‡æ–™å­˜æ”¾äºå°å¤§è³‡ç®¡ç³»ï¼‰æœ‰ä»»ä½•å•é¡Œï¼Œæˆ–æ˜¯ä½ æƒ³åœ¨è‡ªå·±çš„é›»è…¦å˜—è©¦éƒ¨ç½² [Web App](https://share.streamlit.io/icheft/ntuclasscrawler/app.py)ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨ä»¥ä¸‹ä½œæ³•åœ¨ local ç«¯å»ºç½®ã€‚

If the server held in the Department of Information Management fails or you want to deploy this web app locally, here are some tips to get you started.


#### Demo

ğŸ‘‰ğŸ½ [å‚³é€é–€](https://share.streamlit.io/icheft/ntuclasscrawler/app.py) Â· [Web App Link](https://share.streamlit.io/icheft/ntuclasscrawler/app.py)


<img src="./img/desktop_demo.gif" width="100%">

Responsive design as well ğŸ™ŒğŸ¾

<img src="./img/mobile_demo.gif" height='400'>



#### Steps

<div style='font-size: 10px;'>
<p>
* è¨»ï¼šæ­¤éƒ¨åˆ†åƒ…ç‚º local ç«¯ä½ˆå»ºä½¿ç”¨ã€‚<br>
This part serves for people who want to deploy locally.
</p>
</div>




1. Make sure you've installed all the required packages in the desired environment (`pipenv` uses less resources than Anaconda)
2. Run the following command (default)

    ```sh
    python3 run_app.py --deploy
    ```
3. Some optional commands are listed below:

  ```
  optional arguments:
  -h, --help           show this help message and exit
  --semester SEMESTER  Select the semester you want to query
  --toCSV              Export csv file
  -d, --deploy         Deploy your site locally
  -f, --force          Override current course.xlsx file
  ```

  + If `deploy` command is not passed, the app won't be deployed.
  + Beware of the `--force` command as it may take **super long time** to crawl the data from scratch
  + If `course.xlsx` is not found at runtime, the program will automatically start crawling
4. If done successfully, you shall see something like this:

    ```
    course.xlsx file already exists
    Ready to deploy...

      You can now view your Streamlit app in your browser.

      Local URL: http://localhost:8501
      Network URL: http://192.168.2.235:8501
    ```
